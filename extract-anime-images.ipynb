{"cells":[{"cell_type":"markdown","metadata":{},"source":["Welcome to the world of Anime. This code requieres the dataset mentioned in Readme file of github . You can fork the notebook and can use to extract any other parts of the above mentioned site. Let's dive in."]},{"cell_type":"markdown","metadata":{},"source":["The following cells of code will help you to extract the links to each anime character"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# importing the requirements\n","import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","from urllib.request import Request, urlopen\n","import time\n","import os\n","import shutil"]},{"cell_type":"markdown","metadata":{},"source":["What I am doing web scraping. This notebook is also helpful for learning the web scraping."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"outputs":[],"source":["base_link1 = \"https://myanimelist.net/character.php?letter=\"   # base path for every letter (A-Z)\n","base_link2 = \"&show=\"             # base path for every sub pages of given letter (50 anime characters per page)\n","letters_ls = list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n","base_links = [base_link1 + i for i in letters_ls]\n","\n","# these are the number of pages per each alphabet. \n","#For exmaple the anime characters with starting letter \"Z\" are present in 7 pages \n","#(50 per each page and last page may not contain 50 images)\n","num_dict = {}\n","num_dict[\"Z\"] = 7\n","num_dict[\"Y\"] = 67\n","num_dict[\"X\"] = 2\n","num_dict[\"W\"] = 22\n","num_dict[\"V\"] = 13\n","num_dict[\"U\"] = 23\n","num_dict[\"T\"] = 122\n","num_dict[\"S\"] = 186\n","num_dict[\"R\"] = 29\n","num_dict[\"Q\"] = 2\n","num_dict[\"P\"] = 12\n","num_dict[\"O\"] = 62\n","num_dict[\"N\"] = 74\n","num_dict[\"M\"] = 146\n","num_dict[\"L\"] = 26\n","num_dict[\"K\"] = 206\n","num_dict[\"J\"] = 16\n","num_dict[\"I\"] = 61\n","num_dict[\"H\"] = 115\n","num_dict[\"G\"] = 29\n","num_dict[\"F\"] = 41\n","num_dict[\"E\"] = 20\n","num_dict[\"D\"] = 26\n","num_dict[\"C\"] = 31\n","num_dict[\"B\"] = 32\n","num_dict[\"A\"] = 103\n","\n","print(sum(list(num_dict.values())))"]},{"cell_type":"markdown","metadata":{},"source":["This block will give you links to each page of each letter (ie., 7 links for \"Z\" letter)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["final_links = []\n","\n","for num,letter in enumerate(letters_ls):\n","    for i in range(num_dict[letter]):\n","        if i==0:\n","            final_links.append(base_links[num])\n","        else:\n","            final_links.append(base_links[num]+base_link2+str(i*50))\n","len(final_links)"]},{"cell_type":"markdown","metadata":{},"source":["On running below block you will get final links ie., each link contains details of one anime character. Basically I used \n","\n","> time.sleep()\n","\n","to  make sure that there is no much traffic from our side during the web scraping. As we are going through a lot of pages. it is better to use the sleep to make sure that we do not get the [Error 403](https://stackoverflow.com/questions/16627227/http-error-403-in-python-3-web-scraping/31758803)."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["a_tags = []\n","time.sleep(180)\n","for i,link in enumerate(final_links):\n","    headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n","         'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n","         'Referer': 'https://cssspritegenerator.com',\n","         'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n","         'Accept-Encoding': 'none',\n","         'Accept-Language': 'en-US,en;q=0.8',\n","         'Connection': 'keep-alive'}\n","    req = Request(link , headers=headers)\n","    webpage = urlopen(req).read()\n","    soup = BeautifulSoup(webpage, \"html.parser\")\n","    links = soup.findAll(\"a\")\n","    for l in links:\n","        ls = []\n","        if l.img:\n","            a_tags.append(l)\n","    if i%150==0 and i!=0:\n","        print(\"Completed {} links and waiting for 300 seconds\".format(i))\n","        time.sleep(300)\n","        print(\"Waiting completed, going for next set of 150 links\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# This code gives you the actual links form a tag that we got from previous csv file\n","anime_links = []\n","for link in a_tags:\n","    if \"onclick\" in str(link):\n","        a_tags.remove(link)\n","    else:\n","        anime_links.append(link.attrs[\"href\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Creating a new folder where we store the images\n","\n","if os.path.isdir(\"./dataset/\") == False:\n","    os.mkdir(\"./dataset/\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_links = pd.read_csv(\"../input/anime-names-and-image-generation/anime_links.csv\")\n","anime_links = list(df_links[\"Link\"])\n","len(anime_links)     #72992"]},{"cell_type":"markdown","metadata":{},"source":["Following block gives you final images stored in dataset folder (./dataset). You can also extract names of characters form name of each anime image."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","anime_names = []\n","\n","for i,src in enumerate(anime_links):\n","    src_new = src.encode('ascii', 'ignore').decode('ascii')       #there are some names where there are characters other than ascii. To reduce complexity iam excluding those\n","    if src_new != src:\n","        continue\n","    req = Request(src , headers=headers)\n","    webpage = urlopen(req).read()\n","    soup = BeautifulSoup(webpage, \"html.parser\")\n","    links = soup.findAll(\"a\")\n","    for link in links:\n","        if link.img and \"border\" not in str(link) and \"onclick\" not in str(link):  # actual image links\n","            split= str(link.img).split(\"\\\"\")\n","            img_src = split[5]\n","            name = split[1]\n","            \n","            if img_src.split(\".\")[-1] == \"jpg\" and len(name.split(\"/\"))==1:\n","                split = str(link.img).split(\"\\\"\")\n","                response = requests.get(img_src, headers=headers)\n","                image_name = \"./dataset/\"+name + \".jpg\"\n","                file = open(image_name, \"wb\")\n","                file.write(response.content)\n","                file.close()\n","\n","    if i%120==0 and i!=0:  # downloading 120 images and waiting for 4 min and then the loop continues.\n","        print(\"Completed {} links and waiting for 240 seconds\".format(i))\n","        time.sleep(240)\n","        print(\"Waiting completed, going for next set of 120 links\\n\\n\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}
